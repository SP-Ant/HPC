Index: storm-client/pom.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- storm-client/pom.xml	(date 1581779239000)
+++ storm-client/pom.xml	(date 1582315208971)
@@ -104,6 +104,11 @@
             <groupId>org.hamcrest</groupId>
             <artifactId>java-hamcrest</artifactId>
         </dependency>
+        <dependency>
+            <groupId>org.apache.hive</groupId>
+            <artifactId>hive-exec</artifactId>
+            <version>2.3.4</version>
+        </dependency>
     </dependencies>
 
     <build>
Index: storm-server/src/main/java/org/apache/storm/scheduler/WorkerNodeUsageDetails.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- storm-server/src/main/java/org/apache/storm/scheduler/WorkerNodeUsageDetails.java	(date 1585688929935)
+++ storm-server/src/main/java/org/apache/storm/scheduler/WorkerNodeUsageDetails.java	(date 1585688929935)
@@ -0,0 +1,84 @@
+package org.apache.storm.scheduler;
+
+import java.util.Arrays;
+import java.util.List;
+
+public class WorkerNodeUsageDetails {
+
+   /* private final Double CpuUsage;
+    private final Double MemoryUsage;
+    private final String SupervisorID;*/
+
+    public  Double CpuUsage;
+    public  Double MemoryUsage;
+    public  String SupervisorID;
+
+
+    /*public WorkerNodeUsageDetails(Double CpuUsage, Double MemoryUsage, String SupervisorID) {
+
+
+        if (CpuUsage == null) {
+            throw new NullPointerException("MemoryUsage cannot be null");
+        }
+        if (MemoryUsage == null) {
+            throw new NullPointerException("MemoryUsage cannot be null");
+        }
+
+        if (SupervisorID.equals(null)) {
+            throw new NullPointerException("SupervisorID cannot be null");
+        }
+
+        this.CpuUsage = CpuUsage;
+        this.MemoryUsage = MemoryUsage;
+        this.SupervisorID = SupervisorID;
+
+    }
+*/
+
+    public WorkerNodeUsageDetails() {
+
+
+//        if (CpuUsage == null) {
+//            throw new NullPointerException("MemoryUsage cannot be null");
+//        }
+//        if (MemoryUsage == null) {
+//            throw new NullPointerException("MemoryUsage cannot be null");
+//        }
+//
+//        if (SupervisorID.equals(null)) {
+//            throw new NullPointerException("SupervisorID cannot be null");
+//        }
+//
+//        this.CpuUsage = CpuUsage;
+//        this.MemoryUsage = MemoryUsage;
+//        this.SupervisorID = SupervisorID;
+
+    }
+
+
+
+
+
+    public Double getCpuUsage() {
+        return CpuUsage;
+    }
+
+    public Double getMemoryUsage() {
+        return MemoryUsage;
+    }
+
+    public String getSupervisorID() {
+        return SupervisorID;
+    }
+
+    public String getId() {
+        return  getSupervisorID() + getCpuUsage() + ":" + getMemoryUsage();
+    }
+
+    public List<Object> toList() {
+        return Arrays.asList(CpuUsage, MemoryUsage);
+    }
+
+
+
+}
Index: storm-webapp/src/main/java/org/apache/storm/daemon/ui/UIHelpers.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- storm-webapp/src/main/java/org/apache/storm/daemon/ui/UIHelpers.java	(date 1581779239000)
+++ storm-webapp/src/main/java/org/apache/storm/daemon/ui/UIHelpers.java	(date 1586155407895)
@@ -21,87 +21,17 @@
 import com.google.common.base.Joiner;
 import com.google.common.collect.ImmutableMap;
 import com.google.common.collect.Lists;
-import java.io.PrintWriter;
-import java.io.StringWriter;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.Comparator;
-import java.util.EnumSet;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.LinkedList;
-import java.util.List;
-import java.util.Map;
-import java.util.NavigableMap;
-import java.util.Objects;
-import java.util.Set;
-import java.util.concurrent.atomic.AtomicReference;
-import java.util.regex.Matcher;
-import java.util.regex.Pattern;
-import java.util.stream.Collectors;
-import javax.servlet.DispatcherType;
-import javax.servlet.Servlet;
-import javax.ws.rs.core.Response;
-import javax.ws.rs.core.SecurityContext;
 import org.apache.storm.Config;
 import org.apache.storm.Constants;
 import org.apache.storm.DaemonConfig;
-import org.apache.storm.generated.Bolt;
-import org.apache.storm.generated.BoltAggregateStats;
-import org.apache.storm.generated.ClusterSummary;
-import org.apache.storm.generated.CommonAggregateStats;
-import org.apache.storm.generated.ComponentAggregateStats;
-import org.apache.storm.generated.ComponentPageInfo;
-import org.apache.storm.generated.ComponentType;
-import org.apache.storm.generated.ErrorInfo;
-import org.apache.storm.generated.ExecutorAggregateStats;
-import org.apache.storm.generated.ExecutorInfo;
-import org.apache.storm.generated.ExecutorSummary;
-import org.apache.storm.generated.GetInfoOptions;
-import org.apache.storm.generated.GlobalStreamId;
-import org.apache.storm.generated.Grouping;
-import org.apache.storm.generated.KillOptions;
-import org.apache.storm.generated.LogConfig;
-import org.apache.storm.generated.LogLevel;
-import org.apache.storm.generated.LogLevelAction;
-import org.apache.storm.generated.Nimbus;
-import org.apache.storm.generated.NimbusSummary;
-import org.apache.storm.generated.NodeInfo;
-import org.apache.storm.generated.NumErrorsChoice;
-import org.apache.storm.generated.OwnerResourceSummary;
-import org.apache.storm.generated.ProfileAction;
-import org.apache.storm.generated.ProfileRequest;
-import org.apache.storm.generated.RebalanceOptions;
-import org.apache.storm.generated.SpecificAggregateStats;
-import org.apache.storm.generated.SpoutAggregateStats;
-import org.apache.storm.generated.SpoutSpec;
-import org.apache.storm.generated.StormTopology;
-import org.apache.storm.generated.SupervisorPageInfo;
-import org.apache.storm.generated.SupervisorSummary;
-import org.apache.storm.generated.TopologyHistoryInfo;
-import org.apache.storm.generated.TopologyInfo;
-import org.apache.storm.generated.TopologyPageInfo;
-import org.apache.storm.generated.TopologyStats;
-import org.apache.storm.generated.TopologySummary;
-import org.apache.storm.generated.WorkerSummary;
+import org.apache.storm.generated.*;
 import org.apache.storm.logging.filters.AccessLoggingFilter;
 import org.apache.storm.stats.StatsUtil;
-import org.apache.storm.thrift.TException;
-import org.apache.storm.utils.IVersionInfo;
-import org.apache.storm.utils.ObjectReader;
-import org.apache.storm.utils.Time;
-import org.apache.storm.utils.TopologySpoutLag;
-import org.apache.storm.utils.Utils;
-import org.apache.storm.utils.VersionInfo;
-import org.apache.storm.utils.WebAppUtils;
+import org.apache.storm.utils.*;
+import org.apache.thrift.TException;
 import org.eclipse.jetty.http.HttpStatus;
 import org.eclipse.jetty.http.HttpVersion;
-import org.eclipse.jetty.server.HttpConfiguration;
-import org.eclipse.jetty.server.HttpConnectionFactory;
-import org.eclipse.jetty.server.SecureRequestCustomizer;
-import org.eclipse.jetty.server.Server;
-import org.eclipse.jetty.server.ServerConnector;
-import org.eclipse.jetty.server.SslConnectionFactory;
+import org.eclipse.jetty.server.*;
 import org.eclipse.jetty.servlet.FilterHolder;
 import org.eclipse.jetty.servlet.ServletContextHandler;
 import org.eclipse.jetty.servlet.ServletHolder;
@@ -111,6 +41,18 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
+import javax.servlet.DispatcherType;
+import javax.servlet.Servlet;
+import javax.ws.rs.core.Response;
+import javax.ws.rs.core.SecurityContext;
+import java.io.PrintWriter;
+import java.io.StringWriter;
+import java.util.*;
+import java.util.concurrent.atomic.AtomicReference;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+import java.util.stream.Collectors;
+
 @SuppressWarnings("checkstyle:AbbreviationAsWordInName")
 public class UIHelpers {
     private static final Logger LOG = LoggerFactory.getLogger(UIHelpers.class);
Index: storm-server/src/main/java/org/apache/storm/scheduler/MigrationScheduler.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- storm-server/src/main/java/org/apache/storm/scheduler/MigrationScheduler.java	(date 1585090636999)
+++ storm-server/src/main/java/org/apache/storm/scheduler/MigrationScheduler.java	(date 1585090636999)
@@ -0,0 +1,423 @@
+package org.apache.storm.scheduler;
+
+
+import java.util.*;
+
+public class MigrationScheduler {
+
+
+
+    private static ISupervisor supervisor;
+    private static final String SYSTEM_COMPONENT_PREFIX = "__";
+    private static boolean ContinueProgram=true;
+    private static boolean oneTimeAssignment=false;
+
+
+
+
+
+    public static void schedule(Topologies topologies, Cluster cluster) {
+
+        System.out.println("                                                                     ");
+        System.out.println("++++++++++++++++::MigrationScheduler::::schedule:::::: ++++++++++++++++");
+
+
+        Collection <WorkerSlot> workerslot =cluster.getUsedSlots();
+        //int num_worker = 0;
+        int num_worker = workerslot.size();
+        System.out.println("::::::::::num_worker::::::::::: "+num_worker);
+
+        if (num_worker > 0  && oneTimeAssignment == false)
+        {
+
+            oneTimeAssignment = true;
+            System.out.println("++++++++++++++++  before waiting ++++++++++++++++");
+
+
+            try
+            {
+                Thread.sleep(120000);
+            }
+            catch(InterruptedException ex)
+            {
+                Thread.currentThread().interrupt();
+            }
+
+            System.out.println("++++++++++++++++  after waiting ++++++++++++++++");
+
+
+           // return ;
+
+
+        }else
+        {
+            return;
+        }
+
+
+
+
+
+        long start = System.currentTimeMillis();
+        System.out.println("[TIME]" +":"+ "" +  " STARTING SCHEDULING (" + start + ").");
+
+        Map<String, SchedulerAssignment> assignments = cluster.getAssignments();
+
+        System.out.println("[TOPOLOGY] Processing " + assignments.keySet().size() + " topologies.");
+        System.out.println("[TOPOLOGY] Processing assignments  00 " + assignments.toString());
+
+        TopologyDetails topology;
+        SchedulerAssignment assignment;
+
+        // adaptation is done for each topology
+        for (String topologyID : assignments.keySet()) {
+
+            System.out.println( "[TOPOLOGY]:"  + " processing topology " + topologyID);
+
+            // MONITOR
+            // NOTE: sets migration notification on zookeeper for each exec.
+            // As executors are being processed, their migration notification must be unset.
+            //List<AugExecutorContext> moveableExecutors = getMoveableExecutorsContext(topologies.getById(topologyID), topologyContexts.get(topologyID), cluster.getAssignmentById(topologyID));
+            System.out.println("[TOPOLOGY] Processing assignments  11 " );
+            List<AugExecutorContext> moveableExecutors = getMoveableExecutorsContext(cluster,topologies.getById(topologyID),  cluster.getAssignmentById(topologyID));
+            topology = topologies.getById(topologyID);
+            assignment = assignments.get(topologyID);
+
+
+           // System.out.println("[MONITOR] [MOVEABLE] PRINTING MOVEABLE EXECUTORS in " + supervisor.getSupervisorId());
+            System.out.println("[MONITOR] [MOVEABLE] obtained " + moveableExecutors.size() + " executor(s)");
+            System.out.println("[TOPOLOGY] Processing assignments  100 " );
+            System.out.println(" processing topology 100 :::moveableExecutors  " + moveableExecutors.toString());
+
+
+
+
+            for(AugExecutorContext exCtx : moveableExecutors) {
+
+                WorkerSlot localSlot = exCtx.getAugExecutor().getWorkerSlot();
+
+                //System.out.println(" processing topology 101 :::localSlot  " + localSlot);
+               // System.out.println(" processing topology 101 :::localSlot  " + localSlot);
+
+               // System.out.println("[MONITOR] [MOVEABLE] - " + (localSlot == null ? null : localSlot.getPort()) + " " + exCtx.getAugExecutor().getComponentId() + "(" + exCtx.getAugExecutor().getExecutor() + ")");
+                List<ExecutorDetails> source = new ArrayList<ExecutorDetails>();
+                List<ExecutorDetails> target = new ArrayList<ExecutorDetails>();
+                for (AugExecutorDetails aExec : exCtx.getNeighborExecutors()) {
+                    if (exCtx.isTarget(aExec)) target.add(aExec.getExecutor());
+                    else source.add(aExec.getExecutor());
+                }
+                System.out.println("[MONITOR] [MOVEABLE] - - Source: " + source);
+                System.out.println("[MONITOR] [MOVEABLE] - - Target: " + target);
+
+            }
+
+
+
+            for (AugExecutorContext executorContext : moveableExecutors) {
+                //String nodeId = supervisor.getSupervisorId().toString();
+                //hmd
+                Map<String, SupervisorDetails> supervisordetls = cluster.getSupervisors();
+                String nodeId="";
+                for (  Map.Entry<String,  SupervisorDetails > entry : supervisordetls.entrySet()) {
+                    nodeId = entry.getKey();
+                }
+                //hmd
+
+
+
+                int port =6701;
+                WorkerSlot chosenWorkerslot= new WorkerSlot( nodeId,  port);
+                System.out.println("[MONITOR] [MOVEABLE] - - WorkerSlot: 102 " + chosenWorkerslot);
+
+               // if (chosenWorkerslot == null || chosenWorkerslot.getNodeId().equals(supervisor.getSupervisorId())) {
+                if (chosenWorkerslot == null ) {
+
+                    continue;
+                }
+
+                System.out.println("[MONITOR] [MOVEABLE] - - WorkerSlot: 103 " + chosenWorkerslot);
+               // if ( ContinueProgram == true) return;
+
+
+                boolean newAssignmentDone = commitNewExecutorAssignment(
+                        chosenWorkerslot,
+                        executorContext.getAugExecutor(),
+                        topology,
+                        cluster);
+
+                if(newAssignmentDone) {
+                    AugExecutorDetails ex = executorContext.getAugExecutor();
+                    System.out.println("Migration done (sys time: " + System.currentTimeMillis() + ")");
+                    System.out.println("Migrated " + ex.getExecutor() + " " + ex.getComponentId());
+                    System.out.println("From     " + ex.getWorkerSlot());
+                    System.out.println("To       " + chosenWorkerslot);
+
+                     GetExecuterDetails getexecuterdetails = new GetExecuterDetails();
+                     getexecuterdetails.getExecuterMetricDmon(topologies, cluster);
+
+
+                    //topologyCooldownBuffer.set(topologyID);
+                    // won't break here, because other mov executors have been set to migrating
+                    // (check comment at the for-cycle beginning)
+                }
+
+            }
+
+
+        }
+
+        long end = System.currentTimeMillis();
+        long elapsed = end - start;
+      //  System.out.println("[TIME] end of scheduling (" + end + ")");
+      //  System.out.println("[TIME] time elapsed: " + elapsed / 1000 + " seconds and " + elapsed % 1000 + " milliseconds.");
+
+
+
+
+        }
+
+
+
+
+
+    private static boolean commitNewExecutorAssignment(
+            WorkerSlot targetWorkerSlot,
+            AugExecutorDetails executorToMigrate,
+            TopologyDetails topology,
+            Cluster cluster)
+
+    {
+        boolean CONFIG_DEBUG=true;
+
+        SchedulerAssignment assignment = cluster.getAssignmentById(topology.getId());
+        ExecutorDetails executorToMove = executorToMigrate.getExecutor();
+        Map<ExecutorDetails, WorkerSlot> ex2ws = assignment.getExecutorToSlot();
+        System.out.println("[MONITOR] [MOVEABLE] - - ex2ws: 103 " + ex2ws.toString());
+
+        WorkerSlot sourceWorkerSlot = ex2ws.get(executorToMove);
+
+        System.out.println("[MONITOR] [MOVEABLE] - - WorkerSlot: 104 " + executorToMove.toString());
+        System.out.println("[MONITOR] [MOVEABLE] - - WorkerSlot: 105 " + sourceWorkerSlot.toString());
+        System.out.println("::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::");
+
+        List<ExecutorDetails> executorToAssignOnSourceWS = new ArrayList<ExecutorDetails>(),
+                executorToAssignOnTargetWS = new ArrayList<ExecutorDetails>();
+
+        List<WorkerSlot> availableSlots = cluster.getAvailableSlots();
+        String targetslot= "";
+        for (WorkerSlot slot : availableSlots) {
+            String slot_node_id= slot.getNodeId();
+            //int slot_indx= slot.
+            //System.out.println("   ******availableSlots******"+  (slot_node_id) );
+            if (slot_node_id.contains("172.17.245.11"))  targetWorkerSlot=  slot;
+        }
+
+       // System.out.println(" ***targetslot*********"+  (targetslot) );
+       // targetWorkerSlot = targetslot.toString();
+
+
+//        for(ExecutorDetails executor : ex2ws.keySet())
+//
+//        {
+//            System.out.println("[MONITOR] [MOVEABLE] - - executor: 106 " + executor.toString());
+//        }
+
+
+        for(ExecutorDetails executor : ex2ws.keySet())
+
+        {
+
+           // System.out.println("[MONITOR] [MOVEABLE] - - executor: 106 " + executor.toString());
+            // executor to move must be assigned to target worker slot
+            if(executor.equals(executorToMove)) {
+                executorToAssignOnTargetWS.add(executor);
+                continue;
+            }
+
+            WorkerSlot workerSlot = ex2ws.get(executor);
+            //System.out.println("[MONITOR] [MOVEABLE] - - WorkerSlot: 106 workerSlot" + workerSlot.toString());
+
+
+            if(workerSlot.equals(sourceWorkerSlot))
+            {
+                // once source worker slot is freed, executor must be reassigned
+                executorToAssignOnSourceWS.add(executor);
+                //System.out.println("[MONITOR] [MOVEABLE] - - WorkerSlot: 107 workerSlot" + executor.toString());
+            }
+            else if(workerSlot.equals(targetWorkerSlot))
+            {
+                // once target worker slot is freed, executor must be reassigned
+               // System.out.println("[MONITOR] [MOVEABLE] - - WorkerSlot: 108 workerSlot" + executor.toString());
+                executorToAssignOnTargetWS.add(executor);
+            }
+
+
+
+
+            // migration is "done" (differred, but ready), unset migration status
+            //System.out.println("migration is \"done\"");
+           // return true;
+        }
+
+
+        if(CONFIG_DEBUG) {
+
+            System.out.println("[EXECUTE] executors to set on TARGET " + targetWorkerSlot);
+            System.out.println("[EXECUTE] - executorToAssignOnTargetWS  " + executorToAssignOnTargetWS.toString());
+            System.out.println("[EXECUTE] executors to set on LOCAL sourceWorkerSlot  " + sourceWorkerSlot);
+            System.out.println("[EXECUTE] - executorToAssignOnSourceWS  " + executorToAssignOnSourceWS.toString());
+            System.out.println("[EXECUTE] - topology.ID  " + topology.getId().toString());
+            System.out.println("[EXECUTE] committing changes ...");
+
+        }
+
+       // if (ContinueProgram == true) return false;
+        try {
+            // reassign executors to source worker slot
+            cluster.freeSlot(sourceWorkerSlot);
+            if (!executorToAssignOnSourceWS.isEmpty())
+                cluster.assign(sourceWorkerSlot, topology.getId(), executorToAssignOnSourceWS);
+
+            // reassign executors to target worker slot
+            cluster.freeSlot(targetWorkerSlot);
+            if (!executorToAssignOnTargetWS.isEmpty())
+                cluster.assign(targetWorkerSlot, topology.getId(), executorToAssignOnTargetWS);
+        }
+        catch (RuntimeException ex)
+        {
+            System.out.println("[EXECUTE] [RESULT] got exception " + ex.getClass().getSimpleName());
+            ex.printStackTrace();
+            return false;
+        }
+        catch (Exception ex)
+        {
+            System.out.println("[EXECUTE] [RESULT] got exception " + ex.getClass().getSimpleName());
+            ex.printStackTrace();
+            return false;
+        }
+
+
+
+        // System.out.println("[MONITOR] [MOVEABLE] - - WorkerSlot: 107 executorToAssignOnSourceWS" + executorToAssignOnSourceWS.toString());
+      //  System.out.println("::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::");
+      //  System.out.println("[MONITOR] [MOVEABLE] - - WorkerSlot: 108 executorToAssignOnTargetWS" + executorToAssignOnTargetWS.toString());
+
+        //if (ContinueProgram == true) return false;
+        //cluster.
+        System.out.println("migration is \"done\"");
+        return true;
+
+    }
+
+
+
+    //private List<AugExecutorContext> getMoveableExecutorsContext(TopologyDetails topology, GeneralTopologyContext context , SchedulerAssignment assignment)
+    private static List<AugExecutorContext> getMoveableExecutorsContext(Cluster cluster,TopologyDetails topology,  SchedulerAssignment assignment)
+    {
+        List<AugExecutorContext> moveableExecutors = new ArrayList<AugExecutorContext>();
+
+        System.out.println(" processing topology 22 - 00:::  " );
+
+        // preparing stuff
+        if(assignment == null) return moveableExecutors;
+        System.out.println(" processing topology 22 - 11:::  " );
+        Map<ExecutorDetails, WorkerSlot> executorToWorkerSlot = assignment.getExecutorToSlot();
+        System.out.println(" processing topology 22 - 22:::executorToWorkerSlot  " + executorToWorkerSlot.toString() );
+        if(executorToWorkerSlot == null || executorToWorkerSlot.isEmpty()) return moveableExecutors;
+        System.out.println(" processing topology 22 - 33:::localNodeID  " );
+        Map<ExecutorDetails, WorkerSlot> localExecutorToWorkerSlot = new HashMap<ExecutorDetails, WorkerSlot>();
+        System.out.println(" processing topology 22 - 44:::localNodeID  " );
+        //String localNodeID = supervisor.getSupervisorId();
+
+        //hmd
+              // SupervisorDetails supervisor = cluster.getSupervisorById(slot.getNodeId());
+             //  String supervisor_id = supervisor.getId() ;
+           Map<String, SupervisorDetails> supervisordetls = cluster.getSupervisors();
+           String localNodeID="";
+           for (  Map.Entry<String,  SupervisorDetails > entry : supervisordetls.entrySet()) {
+
+                localNodeID = entry.getKey();
+           }
+
+         //hmd
+
+        System.out.println(" processing topology 33 :::localNodeID  " + localNodeID);
+
+
+
+
+        // still preparing..
+        for(ExecutorDetails exec : executorToWorkerSlot.keySet())
+        {
+            WorkerSlot wSlot = executorToWorkerSlot.get(exec);
+
+            System.out.println(" processing topology 44 :::wSlot  " + wSlot);
+
+            if(wSlot != null && localNodeID.equals(wSlot.getNodeId()))
+
+
+                System.out.println(" processing topology 55 :::wSlot  " + wSlot);
+                 System.out.println(" processing topology 66 :::exec  " + exec);
+                localExecutorToWorkerSlot.put(exec, wSlot);
+        }
+
+
+        boolean aRelatedComponentIsMigrating;
+        boolean currentComponentIsLeafNode;
+        boolean currentComponentIsRootNode;
+
+        List<String> sourceComponents;
+        List<String> targetComponents;
+
+        for(ExecutorDetails localExecutor : localExecutorToWorkerSlot.keySet()) {
+            /*	localExecutor cannot be moved if its corresponding localComponent:
+             *  -	is System Component (null id or starts with "__")
+             *  - 	is migrating already
+             *  -   has source or target component which is migrating
+             *  -	is a leaf node (all target are System Component)
+             *  -	is a root node (all source are System Component)
+             */
+            String componentID = topology.getExecutorToComponent().get(localExecutor);
+
+
+           // System.out.println(" processing topology 77 :::componentID  " + componentID);
+
+            // avoid null components and pinned components
+            if(componentID == null || componentID.startsWith(SYSTEM_COMPONENT_PREFIX))
+                continue;
+            System.out.println(" processing topology 77 :::componentID  " + componentID);
+
+            AugExecutorContext augExecContext = new AugExecutorContext(new AugExecutorDetails(localExecutor, topology, assignment));
+            //augExecContext.addNeighbors(topology, context, assignment);
+
+           //  System.out.println(" processing topology 88 :::augExecContext  " + augExecContext.toString());
+
+            ///hmd
+            if (componentID.equals("counter") == true) moveableExecutors.add(augExecContext);
+            /// hmd
+
+            // moveableExecutors.add(augExecContext);
+          //  System.out.println(" processing topology 99 :::augExecContext  " + moveableExecutors.toString());
+
+
+
+
+
+
+        }
+
+         // System.out.println(" processing topology 99 :::augExecContext  " + moveableExecutors.toString());
+        return moveableExecutors;
+    }
+
+
+
+
+
+
+
+    }
+
+
+
+
Index: storm-server/src/main/java/org/apache/storm/scheduler/AugExecutorDetails.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- storm-server/src/main/java/org/apache/storm/scheduler/AugExecutorDetails.java	(date 1585043204457)
+++ storm-server/src/main/java/org/apache/storm/scheduler/AugExecutorDetails.java	(date 1585043204457)
@@ -0,0 +1,41 @@
+package org.apache.storm.scheduler;
+
+public class AugExecutorDetails {
+
+    private ExecutorDetails executor;
+    private String componentId;
+    private WorkerSlot workerSlot;
+
+    public AugExecutorDetails(ExecutorDetails exec, String componentId, WorkerSlot workerSlot)
+    {
+        this.executor = exec;
+        this.componentId = componentId;
+        this.workerSlot = workerSlot;
+    }
+
+    public AugExecutorDetails(ExecutorDetails exec, TopologyDetails topology, SchedulerAssignment assignment)
+    {
+        this.executor = exec;
+        System.out.println(" processing topology 99 :::augExecContext  " + executor.toString());
+        this.componentId = topology.getExecutorToComponent().get(exec);
+        System.out.println(" processing topology 99 :::augExecContext  " + componentId.toString());
+        this.workerSlot = assignment.getExecutorToSlot().get(exec);
+        System.out.println(" processing topology 99 :::augExecContext  " + workerSlot.toString());
+    }
+
+    public ExecutorDetails getExecutor() {
+        return executor;
+    }
+
+    public String getComponentId() {
+        return componentId;
+    }
+
+    public WorkerSlot getWorkerSlot() {
+        return workerSlot;
+    }
+
+    public String getNodeId() {
+        return (workerSlot != null ? workerSlot.getNodeId() : null);
+    }
+}
Index: storm-server/src/main/java/org/apache/storm/scheduler/MonitorScheduling.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- storm-server/src/main/java/org/apache/storm/scheduler/MonitorScheduling.java	(date 1586081799889)
+++ storm-server/src/main/java/org/apache/storm/scheduler/MonitorScheduling.java	(date 1586081799889)
@@ -0,0 +1,413 @@
+package org.apache.storm.scheduler;
+
+
+import org.apache.storm.generated.*;
+import org.apache.storm.stats.StatsUtil;
+import org.apache.storm.thrift.TException;
+import org.apache.storm.utils.Utils;
+
+import java.text.DecimalFormat;
+import java.util.*;
+import java.util.Map.Entry;
+
+
+public class MonitorScheduling {
+
+    private static final String WATCH_TRANSFERRED = "transferred";
+    private static final String WATCH_EMITTED = "emitted";
+
+    private int interval = 4;
+    private String topology;
+    private String topologyID;
+    private String component;
+    private String stream;
+    private String watch;
+
+    public static void call_monitor(){
+
+
+    }
+
+
+
+    public HashSet<String> getComponents(Nimbus.Iface client, String topology) throws Exception {
+        HashSet<String> components = new HashSet<>();
+        ClusterSummary clusterSummary = client.getClusterInfo();
+        TopologySummary topologySummary = null;
+        for (TopologySummary ts : clusterSummary.get_topologies()) {
+            if (topology.equals(ts.get_name())) {
+                topologySummary = ts;
+                break;
+            }
+        }
+        if (topologySummary == null) {
+            throw new IllegalArgumentException("topology: " + topology + " not found");
+        } else {
+            String id = topologySummary.get_id();
+            GetInfoOptions getInfoOpts = new GetInfoOptions();
+            getInfoOpts.set_num_err_choice(NumErrorsChoice.NONE);
+            TopologyInfo info = client.getTopologyInfoWithOpts(id, getInfoOpts);
+            for (ExecutorSummary es : info.get_executors()) {
+                components.add(es.get_component_id());
+            }
+        }
+        return components;
+    }
+
+    public void metrics(Nimbus.Iface client ) throws Exception {
+
+
+        if (interval <= 0) {
+            throw new IllegalArgumentException("poll interval must be positive");
+        }
+
+        if (topology == null || topology.isEmpty()) {
+            throw new IllegalArgumentException("topology name must be something");
+        }
+
+        if (component == null || component.isEmpty()) {
+            HashSet<String> components = getComponents(client, topology);
+            System.out.println("Available components for " + topology + " :");
+            System.out.println("------------------");
+            for (String comp : components) {
+                System.out.println(comp);
+            }
+            System.out.println("------------------");
+            System.out.println("Please use -m to specify one component");
+            return;
+        }
+        if (stream == null || stream.isEmpty()) {
+            throw new IllegalArgumentException("stream name must be something");
+        }
+
+        if (!WATCH_TRANSFERRED.equals(watch) && !WATCH_EMITTED.equals(watch)) {
+            throw new IllegalArgumentException("watch item must either be transferred or emitted");
+        }
+        System.out.println("topology\tcomponent\tparallelism\tstream\ttime-diff ms\t" + watch + "\tthroughput (Kt/s)");
+
+
+        long pollMs = interval * 1000;
+        long now = System.currentTimeMillis();
+        MonitorScheduling.MetricsState state = new MonitorScheduling.MetricsState(now, 0);
+        MonitorScheduling.Poller poller = new MonitorScheduling.Poller(now, pollMs);
+
+
+        do {
+            metrics(client, now, state);
+            /// hd
+            //break;
+            /// hd
+            try {
+                now = poller.nextPoll();
+            } catch (InterruptedException e) {
+            e.printStackTrace();
+             //hmd
+             //break;
+             return;
+             //hmd
+            }
+            break;
+        } while (true);
+    }
+
+    public void metrics(Nimbus.Iface client, long now, MonitorScheduling.MetricsState state) throws Exception {
+        long totalStatted = 0;
+
+        int componentParallelism = 0;
+        boolean streamFound = false;
+
+        ClusterSummary clusterSummary = client.getClusterInfo();
+        TopologySummary topologySummary = null;
+        for (TopologySummary ts : clusterSummary.get_topologies()) {
+            if (topology.equals(ts.get_name())) {
+                topologySummary = ts;
+                break;
+            }
+        }
+        if (topologySummary == null) {
+            throw new IllegalArgumentException("topology: " + topology + " not found");
+        } else {
+            String id = topologySummary.get_id();
+            GetInfoOptions getInfoOpts = new GetInfoOptions();
+            getInfoOpts.set_num_err_choice(NumErrorsChoice.NONE);
+            TopologyInfo info = client.getTopologyInfoWithOpts(id, getInfoOpts);
+
+            for (ExecutorSummary es : info.get_executors()) {
+
+                if (component.equals(es.get_component_id())) {
+                    componentParallelism++;
+                    ExecutorStats stats = es.get_stats();
+                    if (stats != null) {
+                        //System.out.println("::::::::::Going to cll  105::::::stats::::: "+  stats.toString() );
+
+                        Map<String, Map<String, Long>> statted =
+                                WATCH_EMITTED.equals(watch) ? stats.get_emitted() : stats.get_transferred();
+                        if (statted != null) {
+                           // System.out.println("::::::::::statted::::: "+ statted.toString() );
+                           // System.out.println("##################Going to cll  105::::::get_emitted::::: "+ stats.get_emitted().toString());
+                            double rate= stats.get_rate();
+                            ExecutorSpecificStats statsSpecific= stats.get_specific();
+
+                            boolean isbolt =statsSpecific.is_set_bolt();
+
+                             if  (isbolt) {
+
+
+
+                                  //////////////////////////////////
+
+                                 //TopologyInfo topologyInfo = client.getTopologyInfoWithOpts(topoId, getInfoOptions);
+
+                                 Double capacity= getBoltCapacity(client,topologyID,component);
+                                 System.out.println("+++++Capacity::::::::::::::: "+ component+":::::" +capacity ) ;
+                                 //////////////////////////////////////////
+
+
+
+                                 String statsBolt =statsSpecific.get_bolt().get_execute_ms_avg().toString() ;
+
+                                 Map<String, Map<GlobalStreamId, Double>> statsBoltNew = statsSpecific.get_bolt().get_execute_ms_avg();
+
+                                 int c= statsBoltNew.size();
+                                 //System.out.println(":::::::::::::::::statsBolt:::::::::::::::" + component +"::::::"+  statsBolt   );
+
+
+                                 Double executeAverageLatency=0.0;
+                                 for (     Entry<String, Map<GlobalStreamId, Double>> entry : statsBoltNew.entrySet()  ) {
+                                     Map<GlobalStreamId, Double> InfoSection = entry.getValue();
+                                     for ( Entry <GlobalStreamId, Double> entry2 : InfoSection.entrySet()) {
+                                          executeAverageLatency=entry2.getValue();
+                                     }
+                                 }
+
+                                 System.out.println(":::::::::::::::::executeAverageLatency::::::::::::: +" +component +"::::::" + executeAverageLatency   );
+
+
+
+
+
+
+                             }
+
+                            //String resultbolt =  statsSpecific.get_bolt().get_execute_ms_avg().toString();
+
+
+
+                           // System.out.println("::::::::::::::::::::::::::::::stats.get_specific():::::::::::::::::::::::::::::::::::::::" +statsSpecific);
+
+
+                            //System.out.println("::::::::::::::::::::::::::::::resultbolt::::::::::::::::::::::::::::::::::::::" + resultbolt  );
+
+                            System.out.println(":::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::" );
+                          //  System.out.println(":::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::" );
+
+
+                            Map<String, Long> e2 = statted.get(":all-time");
+
+                            if (e2 != null) {
+                                Long stream = e2.get(this.stream);
+                                if (stream != null) {
+                                    streamFound = true;
+                                    totalStatted += stream;
+                                }
+                            }
+                        }
+                    }
+                }
+            }
+
+
+        }
+
+        //System.out.println("::::::::::Going to cll  106::::::stats.get_emitted::::: " );
+        if (componentParallelism <= 0) {
+            HashSet<String> components = getComponents(client, topology);
+            System.out.println("Available components for " + topology + " :");
+            System.out.println("------------------");
+            for (String comp : components) {
+                System.out.println(comp);
+            }
+            System.out.println("------------------");
+            throw new IllegalArgumentException("component: " + component + " not found");
+        }
+
+        if (!streamFound) {
+            throw new IllegalArgumentException("stream: " + stream + " not found");
+        }
+        long timeDelta = now - state.getLastTime();
+        long stattedDelta = totalStatted - state.getLastStatted();
+        state.setLastTime(now);
+        state.setLastStatted(totalStatted);
+        double throughput = (stattedDelta == 0 || timeDelta == 0) ? 0.0 : ((double) stattedDelta / (double) timeDelta);
+        System.out.println(topology + "\t"
+                + component + "\t"
+                + componentParallelism + "\t"
+                + stream + "\t"
+                + timeDelta + "\t"
+                + stattedDelta + "\t"
+                + throughput);
+    }
+
+
+    public static Double  getBoltCapacity(Nimbus.Iface client, String topologyID, String component) throws TException {
+        Double  result=0.0;
+        String topoId=topologyID;
+        boolean sys =false;
+        System.out.println("+++++++++topologyID::::: "+ topologyID );
+        GetInfoOptions getInfoOptions = new GetInfoOptions();
+        getInfoOptions.set_num_err_choice(NumErrorsChoice.ONE);
+        TopologyInfo topologyInfo = client.getTopologyInfoWithOpts(topoId, getInfoOptions);
+        StormTopology stormTopology = client.getTopology(topoId);
+        Map<String, List<ExecutorSummary>> boltSummaries = getBoltExecutors(topologyInfo.get_executors(), stormTopology, sys);
+        //   Map<String, List<ExecutorSummary>> spoutSummaries = getSpoutExecutors(topologyInfo.get_executors(), stormTopology);
+         Map<String, Bolt> boltSpecs = stormTopology.get_bolts();
+         DecimalFormat df2 = new DecimalFormat("#.###");
+
+        //System.out.println("++++++++++boltSpecs::::: "+ boltSpecs.toString() );
+        for (Map.Entry<String, Bolt> boltEntry : boltSpecs.entrySet()) {
+            String boltComponentId = boltEntry.getKey();
+            //System.out.println("++++++++++boltComponentId::::: "+ boltComponentId.toString() );
+            if (boltSummaries.containsKey(boltComponentId) && (sys || !Utils.isSystemId(boltComponentId))) {
+
+                if (!boltComponentId.startsWith("__acker")  && !boltComponentId.startsWith("log")) {
+
+                    if ( boltComponentId.equals(component)) {
+                        result = StatsUtil.computeBoltCapacity(boltSummaries.get(boltComponentId));
+                        String formateed = df2.format(result);
+                        //System.out.println("double : " + String.format("%.2f", input));
+                        Double capacity = Double.parseDouble(formateed);
+                        result = capacity;
+                        //System.out.println(":::::::::Capacity::::: " + ":::::::" + boltComponentId + "::::" + capacity);
+                        //System.out.println("::::::::Capacity::::: " + ":::boltComponentId::::" + boltComponentId + formateed)  ;
+                    }
+
+                }
+
+            }
+
+
+        }
+
+        return result;
+    }
+
+
+
+
+
+
+
+
+    public static Map<String, List<ExecutorSummary>> getBoltExecutors(List<ExecutorSummary> executorSummaries,
+                                                                      StormTopology stormTopology, boolean sys) {
+        Map<String, List<ExecutorSummary>> result = new HashMap();
+        for (ExecutorSummary executorSummary : executorSummaries) {
+            if (StatsUtil.componentType(stormTopology, executorSummary.get_component_id()).equals("bolt")
+                    && (sys || !Utils.isSystemId(executorSummary.get_component_id()))) {
+                List<ExecutorSummary> executorSummaryList = result.getOrDefault(executorSummary.get_component_id(), new ArrayList());
+                executorSummaryList.add(executorSummary);
+                result.put(executorSummary.get_component_id(), executorSummaryList);
+            }
+        }
+        return result;
+    }
+
+
+
+    public void setInterval(int interval) {
+        this.interval = interval;
+    }
+
+    public void setTopology(String topology) {
+        this.topology = topology;
+    }
+
+    public void setTopologyID(String topologyID) {
+        this.topologyID = topologyID;
+    }
+
+
+
+    public void setComponent(String component) {
+        this.component = component;
+    }
+
+    public void setStream(String stream) {
+        this.stream = stream;
+    }
+
+    public void setWatch(String watch) {
+        this.watch = watch;
+    }
+
+    private static class MetricsState {
+        private long lastTime = 0;
+        private long lastStatted = 0;
+
+        private MetricsState(long lastTime, long lastStatted) {
+            this.lastTime = lastTime;
+            this.lastStatted = lastStatted;
+        }
+
+        public long getLastStatted() {
+            return lastStatted;
+        }
+
+        public void setLastStatted(long lastStatted) {
+            this.lastStatted = lastStatted;
+        }
+
+        public long getLastTime() {
+            return lastTime;
+        }
+
+        public void setLastTime(long lastTime) {
+            this.lastTime = lastTime;
+        }
+    }
+
+    private static class Poller {
+        private long startTime = 0;
+        private long pollMs = 0;
+
+        private Poller(long startTime, long pollMs) {
+            this.startTime = startTime;
+            this.pollMs = pollMs;
+        }
+
+        public long nextPoll() throws InterruptedException {
+            long now = System.currentTimeMillis();
+            long cycle = (now - startTime) / pollMs;
+            long wakeupTime = startTime + (pollMs * (cycle + 1));
+            long sleepTime = wakeupTime - now;
+            if (sleepTime > 0) {
+                Thread.sleep(sleepTime);
+            }
+            now = System.currentTimeMillis();
+            return now;
+        }
+
+        public long getStartTime() {
+            return startTime;
+        }
+
+        public void setStartTime(long startTime) {
+            this.startTime = startTime;
+        }
+
+        public long getPollMs() {
+            return pollMs;
+        }
+
+        public void setPollMs(long pollMs) {
+            this.pollMs = pollMs;
+        }
+    }
+
+
+
+
+
+
+
+}
Index: storm-server/src/main/java/org/apache/storm/scheduler/AugExecutorContext.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- storm-server/src/main/java/org/apache/storm/scheduler/AugExecutorContext.java	(date 1584990741519)
+++ storm-server/src/main/java/org/apache/storm/scheduler/AugExecutorContext.java	(date 1584990741519)
@@ -0,0 +1,70 @@
+package org.apache.storm.scheduler;
+
+import org.apache.storm.generated.GlobalStreamId;
+import org.apache.storm.generated.Grouping;
+import org.apache.storm.task.GeneralTopologyContext;
+
+import java.util.*;
+
+public class AugExecutorContext {
+
+    private static final String  SYSTEM_COMPONENT_PREFIX = "__";
+
+    private AugExecutorDetails augExecutor;
+    List<AugExecutorDetails> neighborExecutors = new ArrayList<AugExecutorDetails>();
+    Map<ExecutorDetails, Boolean> isTargetMapping = new HashMap<ExecutorDetails, Boolean>();
+
+
+    public AugExecutorContext(AugExecutorDetails augExecutor)
+    {
+        this.augExecutor = augExecutor;
+    }
+
+    public AugExecutorDetails getAugExecutor() {
+        return augExecutor;
+    }
+
+
+    private List<String> extractSourceComponentsId(String componentId, GeneralTopologyContext context)
+    {
+        List<String> sourceComponentsId = new ArrayList<String>();
+
+        Map<GlobalStreamId, Grouping> sources = context.getSources(componentId);
+        for(GlobalStreamId globalStreamID : sources.keySet()){
+            if (globalStreamID!=null && !globalStreamID.get_componentId().startsWith(SYSTEM_COMPONENT_PREFIX))
+                sourceComponentsId.add(globalStreamID.get_componentId());
+        }
+
+        return sourceComponentsId;
+    }
+
+    private List<String> extractTargetComponentsId(String componentId, GeneralTopologyContext context)
+    {
+        List<String> targetComponentsId = new ArrayList<String>();
+
+        Map<String, Map<String, Grouping>> targets = context.getTargets(componentId);
+
+        for(String streamId : targets.keySet()){
+            Set<String> componentsId = targets.get(streamId).keySet();
+
+            if (streamId!=null && streamId.equals("default"))
+                targetComponentsId.addAll(componentsId);
+        }
+
+        return targetComponentsId;
+    }
+
+
+    public List<AugExecutorDetails> getNeighborExecutors() {
+        return neighborExecutors;
+    }
+
+    public Boolean isTarget(AugExecutorDetails exec)
+    {
+        return isTargetMapping.get(exec.getExecutor());
+    }
+
+
+
+
+}
Index: storm-server/src/main/java/org/apache/storm/scheduler/Cluster.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- storm-server/src/main/java/org/apache/storm/scheduler/Cluster.java	(date 1581779239000)
+++ storm-server/src/main/java/org/apache/storm/scheduler/Cluster.java	(date 1584856724313)
@@ -18,17 +18,6 @@
 
 package org.apache.storm.scheduler;
 
-import java.util.ArrayList;
-import java.util.Collection;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Map;
-import java.util.Map.Entry;
-import java.util.Set;
-import java.util.function.Function;
-import java.util.stream.Collectors;
 import org.apache.storm.Config;
 import org.apache.storm.Constants;
 import org.apache.storm.DaemonConfig;
@@ -49,6 +38,10 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
+import java.util.*;
+import java.util.Map.Entry;
+import java.util.stream.Collectors;
+
 /**
  * The current state of the storm cluster.  Cluster is not currently thread safe.
  */
@@ -312,14 +305,51 @@
 
     @Override
     public List<TopologyDetails> needsSchedulingTopologies() {
+
         List<TopologyDetails> ret = new ArrayList<>();
         for (TopologyDetails topology : getTopologies()) {
-            if (needsScheduling(topology)) {
+          //  if (needsScheduling(topology)  ) {
+            System.out.println("Cluster:::::::needsSchedulingTopologies ::added:::Topology:: "+ EvenScheduler.check_cpu_usae );
+            if (needsScheduling(topology) || EvenScheduler.check_cpu_usae == true ) {
+                String topologyId = topology.getId();
+                System.out.println("Cluster:::::::needsSchedulingTopologies ::added:::Topology:: "+topologyId.toString()+":::.allow_reschedule::"+EvenScheduler.allow_reschedule );
                 ret.add(topology);
             }
         }
 
         return ret;
+
+//hmd
+        //        List<TopologyDetails> ret = new ArrayList<>();
+//        for (TopologyDetails topology : getTopologies()) {
+//
+//
+//            boolean allow_spec_top=false;
+//
+//            String topologyId = topology.getId();
+//            if ( topologyId.contains("MyTopology2") ) {
+//                 allow_spec_top =true;
+//                System.out.println(":::::::::::Cluster::::::::::allow_spec_top::::::::: " +  allow_spec_top );
+//            }
+//
+//            if (    needsScheduling(topology)  ||   (   EvenScheduler.one_time_reschedule ==true &&  EvenScheduler.allow_reschedule ==true &&  allow_spec_top == true)  ) {
+//
+//                if ( ((EvenScheduler.one_time_reschedule ==true &&  EvenScheduler.allow_reschedule ==true &&  allow_spec_top == true) ) ) {
+//                    System.out.println("*****************3 condition true************************* " + topologyId );
+//                    EvenScheduler.one_time_reschedule=false;
+//                    ret.add(topology);
+//                }
+//
+//                System.out.println(":::Cluster::::::inside:::one_time_reschedule::: " +  EvenScheduler.one_time_reschedule+":::allow_reschedule::: " +  EvenScheduler.allow_reschedule +":::allow_spec_top::: " +  allow_spec_top);
+//                EvenScheduler.one_time_reschedule=false;
+//                ret.add(topology);
+//
+//            }
+//        }
+//
+//        return ret;
+
+//hmd
     }
 
     @Override
Index: storm-server/src/main/java/org/apache/storm/scheduler/IsolationScheduler.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- storm-server/src/main/java/org/apache/storm/scheduler/IsolationScheduler.java	(date 1581779239000)
+++ storm-server/src/main/java/org/apache/storm/scheduler/IsolationScheduler.java	(date 1583291472772)
@@ -65,6 +65,8 @@
     // set blacklist to what it was initially
     @Override
     public void schedule(Topologies topologies, Cluster cluster) {
+
+        System.out.println("++++++++++++++++  Isolion scheduler ++++++++++++++++");
         Set<String> origBlacklist = cluster.getBlacklistedHosts();
         List<TopologyDetails> isoTopologies = isolatedTopologies(topologies.getTopologies());
         Set<String> isoIds = extractTopologyIds(isoTopologies);
Index: storm-server/src/main/java/org/apache/storm/scheduler/GetWorkerNodeInfo.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- storm-server/src/main/java/org/apache/storm/scheduler/GetWorkerNodeInfo.java	(date 1585694051984)
+++ storm-server/src/main/java/org/apache/storm/scheduler/GetWorkerNodeInfo.java	(date 1585694051984)
@@ -0,0 +1,206 @@
+package org.apache.storm.scheduler;
+
+
+import com.jcraft.jsch.Channel;
+import com.jcraft.jsch.ChannelExec;
+import com.jcraft.jsch.JSch;
+import com.jcraft.jsch.Session;
+
+import java.io.BufferedReader;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.InputStreamReader;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Map;
+import java.util.Properties;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+public class GetWorkerNodeInfo {
+
+   // private  List<WorkerNodeUsageDetails> workerNodeUsageDetails = null;
+
+  //  public static List<WorkerNodeUsageDetails> workerNodeUsageDetails = new ArrayList<WorkerNodeUsageDetails>();
+    public static List<WorkerNodeUsageDetails> workerNodeUsageDetails = null;
+
+    public static List<WorkerSlot> workerSlots = new ArrayList<WorkerSlot>();
+
+
+
+    public static List<WorkerNodeUsageDetails> getWorkernodeInfo(Topologies topologies, Cluster cluster) {
+
+       // List<SupervisorDetails> supervisor = cluster.getSupervisorById();
+         //String supervisor_id = supervisor.getId() ;
+
+
+                 try
+                {
+                    Thread.sleep(20000);
+                }
+                catch(InterruptedException ex)
+                {
+                    Thread.currentThread().interrupt();
+                }
+
+                System.out.println("++++++++++++++++  after waiting ++++++++++++++++");
+
+        workerNodeUsageDetails = new ArrayList<WorkerNodeUsageDetails>();
+        Map<String, SupervisorDetails> supervisordetls = cluster.getSupervisors();
+
+        for (  Map.Entry<String,  SupervisorDetails > entry : supervisordetls.entrySet()) {
+            String id_supervisor = entry.getKey();
+            //System.out.println("*************id_supervisor****entry**********"+  (id_supervisor.toString()) );
+            String extractedIP= extractIPfromSupervisorID(id_supervisor);
+            getUsageInfoFromHosts(extractedIP, id_supervisor);
+            //String[] arrOfStr = id_supervisor.split(".");
+            //System.out.println("*************extractedIP**********"+extractedIP);
+        }
+
+
+        //display the list
+//        List<WorkerNodeUsageDetails> workerNodeUsageDetails = null;
+//        GetWorkerNodeInfo getworkernodeInfo  =new GetWorkerNodeInfo();
+//        workerNodeUsageDetails= getworkernodeInfo.getWorkernodeInfo( topologies,  cluster);
+//
+//        for (WorkerNodeUsageDetails workernodedetails : workerNodeUsageDetails) {
+//
+//            System.out.println("+++++++++++ ::DefaultScheduler:::: ++++++++CpuUsage+++++++++++"+workernodedetails.CpuUsage);
+//            System.out.println("++++++++++++::DefaultScheduler::::: ++++++++MemoryUsage++++++++"+workernodedetails.MemoryUsage);
+//            System.out.println("++++++++++++::DefaultScheduler:::::: ++++++++SupervisorID++++++"+workernodedetails.SupervisorID);
+//
+//        }
+
+
+       return workerNodeUsageDetails;
+
+
+
+
+    }
+
+
+
+
+    public static void getUsageInfoFromHosts(String IP, String id_supervisor) {
+
+        Double CpuUsage=0.0;
+        Double MemoryUsage=0.0;
+
+        try{
+            //String command = "ls -la";
+            //String command = "top";
+            //String command = "ls -l";
+            //String command = " top -b -d1 -n1|grep -i \"Cpu(s)\"|head -c21|cut -d ' ' -f3|cut -d '%' -f1";
+            //String command = " iostat";
+            String command = "top -b -n1";
+            // String command = "top -i";
+            String host = IP;
+            String user = "ali";
+            String password = "cheraakhe";
+            JSch jsch = new JSch();
+            Session session = jsch.getSession(user, host, 22);
+            Properties config = new Properties();
+            config.put("StrictHostKeyChecking", "no");
+            session.setConfig(config);;
+            session.setPassword(password);
+            session.connect();
+            Channel channel = session.openChannel("exec");
+            ((ChannelExec)channel).setCommand(command);
+            channel.setInputStream(null);
+            ((ChannelExec)channel).setErrStream(System.err);
+            InputStream input = channel.getInputStream();
+            channel.connect();
+            //System.out.println("Channel Connected to machine " + host + " server with command: " + command );
+            try{
+                InputStreamReader inputReader = new InputStreamReader(input);
+                BufferedReader bufferedReader = new BufferedReader(inputReader);
+                String line = null;
+
+                while((line = bufferedReader.readLine()) != null){
+                   // System.out.println(line);
+
+
+
+                    if (line.startsWith("top") ){
+
+
+                        String result = line.substring(line.indexOf("load average:") , line.length());
+                        result=result.replace("load average:","");
+
+                        String[] arrOfStr = result.split(",");
+                        String tmp=arrOfStr[1].toString();
+                        CpuUsage= Double.parseDouble(tmp);
+                        //System.out.println("::cpuusage::IP"+ IP + " :::" + CpuUsage);
+
+
+
+
+                    }
+
+                    if (line.startsWith("KiB Swap") ) {
+                        String result = line.substring(line.indexOf("used"), line.length());
+                        result = result.replace("used.", "");
+                        result = result.replace("avail Mem", "");
+                        result = result.trim();
+
+                        MemoryUsage= Double.parseDouble(result);
+                        //System.out.println("::MemoryUsage::+IP" + IP +  ":::::" + MemoryUsage);
+                    }
+
+
+
+
+
+                   }
+
+                bufferedReader.close();
+                inputReader.close();
+
+                WorkerNodeUsageDetails workerDetails = new WorkerNodeUsageDetails();
+                workerDetails.CpuUsage=CpuUsage;
+                workerDetails.MemoryUsage=MemoryUsage;
+                workerDetails.SupervisorID=id_supervisor;
+
+
+                workerNodeUsageDetails.add(workerDetails);
+
+
+
+
+
+
+            }catch(IOException ex){
+                ex.printStackTrace();
+            }
+
+            channel.disconnect();
+            session.disconnect();
+        }catch(Exception ex){
+            ex.printStackTrace();
+        }
+
+
+    }
+
+
+    public  static  String extractIPfromSupervisorID(String SupervisorID) {
+
+        String IPADDRESS_PATTERN =
+                "(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)";
+
+        Pattern pattern = Pattern.compile(IPADDRESS_PATTERN);
+        Matcher matcher = pattern.matcher(SupervisorID);
+        if (matcher.find()) {
+            return matcher.group();
+            //System.out.println("*************Found**********"+ matcher.group().toString());
+        } else{
+            return "0.0.0.0";
+            //System.out.println("*************Not Found*********"+ matcher.group().toString());
+        }
+
+
+
+
+    }
+}
Index: storm-server/src/main/java/org/apache/storm/scheduler/EvenScheduler.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- storm-server/src/main/java/org/apache/storm/scheduler/EvenScheduler.java	(date 1581779239000)
+++ storm-server/src/main/java/org/apache/storm/scheduler/EvenScheduler.java	(date 1584856296745)
@@ -18,25 +18,22 @@
 
 package org.apache.storm.scheduler;
 
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.Comparator;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
-import java.util.TreeMap;
+import com.google.common.collect.Sets;
 import org.apache.storm.shade.com.google.common.annotations.VisibleForTesting;
-import org.apache.storm.shade.com.google.common.collect.Sets;
 import org.apache.storm.utils.ServerUtils;
 import org.apache.storm.utils.Utils;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
+import java.util.*;
+
+//import org.apache.storm.shade.com.google.common.collect.Sets;
+
 public class EvenScheduler implements IScheduler {
     private static final Logger LOG = LoggerFactory.getLogger(EvenScheduler.class);
-
+   // public static boolean allow_reschedule = false;
+    public static boolean allow_reschedule = true;
+    public static boolean check_cpu_usae = false;
     @VisibleForTesting
     public static List<WorkerSlot> sortSlots(List<WorkerSlot> availableSlots) {
         //For example, we have a three nodes(supervisor1, supervisor2, supervisor3) cluster:
@@ -50,6 +47,16 @@
         //supervisor3:6702, supervisor2:6702,
         //supervisor3:6703
 
+        System.out.println("scheduleTopologiesEvenly::::::::sortSlots::::: ") ;
+
+       /// hmd
+//        for (WorkerSlot slot : availableSlots) {
+//            String slot_node_id= slot.getNodeId();
+//            //int slot_indx= slot.
+//            System.out.println("  EvenScheduler ***sortSlots***availableSlots******"+  (slot_node_id) );
+//        }
+       ///hmd
+
         if (availableSlots != null && availableSlots.size() > 0) {
             // group by node
             Map<String, List<WorkerSlot>> slotGroups = new TreeMap<>();
@@ -58,9 +65,11 @@
                 List<WorkerSlot> slots = null;
                 if (slotGroups.containsKey(node)) {
                     slots = slotGroups.get(node);
+                   // System.out.println("  EvenScheduler ***sortSlots***if  00******"+  (slots.toString()) );
                 } else {
                     slots = new ArrayList<WorkerSlot>();
                     slotGroups.put(node, slots);
+                   // System.out.println("  EvenScheduler ***sortSlots***else  01******"+  (slots.toString()) );
                 }
                 slots.add(slot);
             }
@@ -70,6 +79,8 @@
                 Collections.sort(slots, new Comparator<WorkerSlot>() {
                     @Override
                     public int compare(WorkerSlot o1, WorkerSlot o2) {
+                        //System.out.println("  EvenScheduler ***sortSlots***else  02******"+  (slots.toString()) );
+                       // System.out.println("  EvenScheduler ***sortSlots***else  03******" + o1.toString() + "::::" +o1.toString());
                         return o1.getPort() - o2.getPort();
                     }
                 });
@@ -79,7 +90,11 @@
             List<List<WorkerSlot>> list = new ArrayList<List<WorkerSlot>>(slotGroups.values());
             Collections.sort(list, new Comparator<List<WorkerSlot>>() {
                 @Override
+
                 public int compare(List<WorkerSlot> o1, List<WorkerSlot> o2) {
+
+                   // System.out.println("  EvenScheduler ***sortSlots***else  04******" +o2.size()+"::::"+o1.size());
+
                     return o2.size() - o1.size();
                 }
             });
@@ -101,54 +116,170 @@
     }
 
     private static Map<ExecutorDetails, WorkerSlot> scheduleTopology(TopologyDetails topology, Cluster cluster) {
-        List<WorkerSlot> availableSlots = cluster.getAvailableSlots();
+       /// orig  List<WorkerSlot> availableSlots = cluster.getAvailableSlots();
+
+        ///hmd
+       // System.out.println("*******  EvenScheduler **scheduleTopology****slots_full****00000**"+ ":::allow_reschedule::::" + allow_reschedule);
+
+        List<WorkerSlot> slots = cluster.getAvailableSlots();
+       // System.out.println("*******  EvenScheduler **scheduleTopology****slots_full****11111**"+ ":::allow_reschedule::::" + allow_reschedule);
+
+
+        List<WorkerSlot> availableSlots = null;
+        availableSlots = new ArrayList<WorkerSlot>();
+
+        for (WorkerSlot slot : slots) {
+
+            String slot_node_id= slot.getNodeId();
+
+            //System.out.println("*******  EvenScheduler **scheduleTopology****slots_full******"+  (slot_node_id) +":::allow_reschedule::::" + allow_reschedule);
+
+
+           if (allow_reschedule == true) {
+
+
+                if (slot_node_id.contains("172.17.245.10")) {
+                    System.out.println("********  EvenScheduler **scheduleTopology***inside if *******" + ":::allow_reschedule::::" + allow_reschedule);
+                    availableSlots.add(slot);
+                }
+
+           }else
+           {
+               if (slot_node_id.contains("172.17.245.11")) {
+                   System.out.println("********  EvenScheduler **scheduleTopology***inside if *******" + ":::allow_reschedule::::" + allow_reschedule);
+                   availableSlots.add(slot);
+               }
+
+
+
+           }
+
+
+        }
+
+
         Set<ExecutorDetails> allExecutors = topology.getExecutors();
         Map<WorkerSlot, List<ExecutorDetails>> aliveAssigned = getAliveAssignedWorkerSlotExecutors(cluster, topology.getId());
         int totalSlotsToUse = Math.min(topology.getNumWorkers(), availableSlots.size() + aliveAssigned.size());
 
         List<WorkerSlot> sortedList = sortSlots(availableSlots);
+
         if (sortedList == null) {
             LOG.error("No available slots for topology: {}", topology.getName());
             return new HashMap<ExecutorDetails, WorkerSlot>();
         }
 
+
+        for (WorkerSlot sorted : sortedList) {
+            String sorted_id= sorted.getNodeId() ;
+            //int slot_indx= slot.
+
+        }
+
+        //System.out.println("  EvenScheduler ***sorted_id***availableSlots******"+  (sortedList.toString()) );
+
         //allow requesting slots number bigger than available slots
         int toIndex = (totalSlotsToUse - aliveAssigned.size())
-                      > sortedList.size() ? sortedList.size() : (totalSlotsToUse - aliveAssigned.size());
+                > sortedList.size() ? sortedList.size() : (totalSlotsToUse - aliveAssigned.size());
         List<WorkerSlot> reassignSlots = sortedList.subList(0, toIndex);
 
+        for (WorkerSlot reassignslots : reassignSlots) {
+            String reassign= reassignslots.getNodeId() ;
+            //int slot_indx= slot.
+           // System.out.println("  EvenScheduler ***sorted_id***reassign******"+  (reassign) );
+        }
+        //System.out.println("  EvenScheduler ***sorted_id***reassignslots******"+  (reassignSlots.toString()) );
+
+
         Set<ExecutorDetails> aliveExecutors = new HashSet<ExecutorDetails>();
         for (List<ExecutorDetails> list : aliveAssigned.values()) {
+            String alivessigned= aliveAssigned.toString() ;
+            //System.out.println("  EvenScheduler ***sorted_id***alivessigned******"+  (alivessigned) );
             aliveExecutors.addAll(list);
         }
-        Set<ExecutorDetails> reassignExecutors = Sets.difference(allExecutors, aliveExecutors);
+         // System.out.println("  EvenScheduler ***sorted_id***alivessigned******"+  (aliveExecutors) );
+
+           Set<ExecutorDetails> reassignExecutors = Sets.difference(allExecutors, aliveExecutors);
+          //Set<ExecutorDetails> reassignExecutors = (allExecutors);
+
+//            for (ExecutorDetails resginexec : reassignExecutors) {
+//                String resign_str= resginexec.toString() ;
+//                System.out.println("  EvenScheduler ***sorted_id***reassignExecutors******"+  (resign_str) );
+//
+//            }
+       // System.out.println("  EvenScheduler ***sorted_id***reassignExecutors******"+  (allExecutors.toString()) );
+       // System.out.println("  EvenScheduler ***sorted_id***aliveExecutors******"+  (aliveExecutors.toString()) );
+
+
 
         Map<ExecutorDetails, WorkerSlot> reassignment = new HashMap<ExecutorDetails, WorkerSlot>();
+
+
+
+
+/// originl
+
         if (reassignSlots.size() == 0) {
+
+            //System.out.println("  EvenScheduler ***sorted_id***reassignSlots.size()******"+  (reassignSlots.size()) );
+            //System.out.println("  EvenScheduler ***sorted_id***reassignSlots.text()******" +  reassignSlots.toString() );
+
             return reassignment;
         }
 
+
+/// originl
+
+
         List<ExecutorDetails> executors = new ArrayList<ExecutorDetails>(reassignExecutors);
+
+        for (ExecutorDetails exec : executors) {
+           // String exec_str= exec.toString();
+            //int slot_indx= slot.
+           // System.out.println("  EvenScheduler ***exec_str******"+  (exec_str) );
+        }
+
+
         Collections.sort(executors, new Comparator<ExecutorDetails>() {
             @Override
             public int compare(ExecutorDetails o1, ExecutorDetails o2) {
+
+               // System.out.println("  EvenScheduler ***01******"+  o1.toString()+"***02******"+o2.toString()  );
                 return o1.getStartTask() - o2.getStartTask();
             }
         });
 
+
+
+
         for (int i = 0; i < executors.size(); i++) {
             reassignment.put(executors.get(i), reassignSlots.get(i % reassignSlots.size()));
+            //System.out.println("  EvenScheduler executors.size {}******"+ executors.size());
+            //System.out.println("  EvenScheduler executors.get(i)******"+ executors.get(i));
+
         }
 
         if (reassignment.size() != 0) {
             LOG.info("Available slots: {}", availableSlots.toString());
+            //System.out.println("  EvenScheduler *LOG**Available slots: {}******"+ availableSlots.toString());
         }
         return reassignment;
     }
 
     public static void scheduleTopologiesEvenly(Topologies topologies, Cluster cluster) {
+
+
         for (TopologyDetails topology : cluster.needsSchedulingTopologies()) {
             String topologyId = topology.getId();
+
+            if (check_cpu_usae == true) {
+                cluster.unassign(topologyId);
+                System.out.println("scheduleTopologiesEvenly::::::::Cluster check_cpu_usage set to false::::: ") ;
+                check_cpu_usae =false;
+            }
+
+
+            System.out.println("scheduleTopologiesEvenly::::::::topologyId::::: "+topologyId.toString()) ;
             Map<ExecutorDetails, WorkerSlot> newAssignment = scheduleTopology(topology, cluster);
             Map<WorkerSlot, List<ExecutorDetails>> nodePortToExecutors = Utils.reverseMap(newAssignment);
 
@@ -156,6 +287,8 @@
                 WorkerSlot nodePort = entry.getKey();
                 List<ExecutorDetails> executors = entry.getValue();
                 cluster.assign(nodePort, topologyId, executors);
+                //cluster.un
+
             }
         }
     }
@@ -175,4 +308,4 @@
         return new HashMap<>();
     }
 
-}
+}
\ No newline at end of file
Index: storm-server/src/main/java/org/apache/storm/scheduler/Hmetrics.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- storm-server/src/main/java/org/apache/storm/scheduler/Hmetrics.java	(date 1585807618869)
+++ storm-server/src/main/java/org/apache/storm/scheduler/Hmetrics.java	(date 1585807618869)
@@ -0,0 +1,689 @@
+package org.apache.storm.scheduler;
+
+
+import java.util.*;
+
+//import org.apache.storm.shade.com.google.common.collect.Sets;
+//import org.apache.storm.utils.ConfigUtils;
+//import org.apache.storm.utils.Monitor;
+
+//import static jodd.util.ThreadUtil.sleep;
+
+//import javax.ws.rs.core.Response;
+
+
+public  class Hmetrics {
+
+    //private static final Logger LOG = LoggerFactory.getLogger(EvenScheduler.class);
+   // public static Map<String, Object> config = ConfigUtils.readStormConfig();
+
+    //static List<String> lst_topologies = new ArrayList<String>();
+    private static final String WATCH_TRANSFERRED = "transferred";
+    private static final String WATCH_EMITTED = "emitted";
+
+    private int interval = 4;
+    private String component;
+    private static String stream;
+    //private String watch;
+    private static String watch;
+
+    //stream
+
+    public static void display_metrics(Topologies topologies, Cluster cluster) {
+
+        System.out.println("                                                                     ");
+        System.out.println("++++++++++++++++::Hmetrics::::display_metrics:::::: ++++++++++++++++");
+
+       // MigrationScheduler migrationScheduler  =new MigrationScheduler();
+       // migrationScheduler.schedule( topologies,  cluster);
+
+       /* MonitorScheduling monitor =new MonitorScheduling();
+        monitor.call_monitor();
+
+        Integer interval=4;
+        String component="null";
+        String stream ="default";
+        String watch ="emitted";
+        String topologyName="MyTopology";
+
+        monitor.setComponent(component);
+        monitor.setStream(stream);
+        monitor.setWatch(watch);
+        monitor.setTopology(topologyName);
+*/
+
+      /*  Map<String, Object> cl = CLI.opt("i", "interval", 4, CLI.AS_INT)
+                .opt("m", "component", null)
+                .opt("s", "stream", "default")
+                .opt("w", "watch", "emitted")
+                .arg("topologyName", CLI.FIRST_WINS)
+                .parse(args);
+//       *////Integer interval = (Integer) cl.get("i");
+//        if (null != interval) {
+//            monitor.setInterval(interval);
+//        }
+
+
+
+//        String component = (String) cl.get("m");
+//        if (null != component) {
+//            monitor.setComponent(component);
+//        }
+       /* String stream = (String) cl.get("s");
+        if (null != stream) {
+            monitor.setStream(stream);
+        }
+        String watch = (String) cl.get("w");
+        if (null != watch) {
+            monitor.setWatch(watch);
+        }
+        String topologyName = (String) cl.get("topologyName");
+        if (null != topologyName) {
+            monitor.setTopology(topologyName);
+        }
+*/
+
+
+
+
+        /*try {
+
+            NimbusClient.withConfiguredClient(new NimbusClient.WithNimbus() {
+              @Override
+              public void run(Nimbus.Iface nimbus) throws Exception {
+                  monitor.metrics(nimbus);
+              }
+          });
+        } catch (Exception e) {
+            e.printStackTrace();
+        }*/
+
+
+        //Cluster cluster = new Cluster();
+        //hmetrics.display_metrics();
+
+         ///// a
+        //Monitor monitor =new Monitor();
+        //monitor.
+
+        //monitor.metrics();
+
+        List<WorkerSlot> availableSlots = cluster.getAvailableSlots();
+
+        Collection<WorkerSlot> used_ports = cluster.getUsedSlots();
+
+        Set<WorkerSlot> slots= new HashSet<WorkerSlot>();
+
+        //System.out.println("*****************  slots *****************"+  Integer.toString(slots.size()) );
+
+        for (WorkerSlot port : used_ports)
+            System.out.println("::::::::::USED_SLOTS::::::::::::: " + port.toString());
+
+
+        for (TopologyDetails  topology : cluster.getTopologies()) {
+
+                    String  topologyId =topology.getId().toString();
+
+
+
+                    Set<ExecutorDetails> allExecutors = topology.getExecutors();
+
+
+                   // String id = topologySummary.get_id();
+
+                     //inim client =new NimbusClient();
+            //client.gett
+                     //Nimbus.Iface  client  =   new Nimbus.Iface(){};
+                   // Nimbus.Iface  client  =   new Nimbus.Iface(){
+
+
+                   // }
+                    // client.getClusterInfo();
+                /*    Nimbus.Iface client;
+                    long totalStatted = 0;
+
+                    int componentParallelism = 0;
+                    boolean streamFound = false;
+                    System.out.println(":::::::::H:metrics:::metrics::00:::::::: " );
+
+                    String id = topologyId;
+                    GetInfoOptions getInfoOpts = new GetInfoOptions();
+                    getInfoOpts.set_num_err_choice(NumErrorsChoice.NONE);
+                    TopologyInfo info = client.getTopologyInfoWithOpts(id, getInfoOpts);
+                    System.out.println(":::::::::H:metrics:::metrics::11:::::::: " );
+                    // TopologyInfo info = client.getTopologyInfoWithOpts(id, getInfoOpts);
+
+                    for (ExecutorSummary es : info.get_executors()) {
+                        //if (component.equals(es.get_component_id())) {
+                         //   componentParallelism++;
+                        //    ExecutorStats stats = es.get_stats();
+                       // //System.out.println(":::::::::H:metrics:::metrics::22:::::::: " );
+                            if (stats != null) {
+                            //    Map<String, Map<String, Long>> statted =
+                              //          WATCH_EMITTED.equals(watch) ? stats.get_emitted() : stats.get_transferred();
+                             //   if (statted != null) {
+                             //       Map<String, Long> e2 = statted.get(":all-time");
+                                    if (e2 != null) {
+                              //          Long stream = e2.get(stream);
+                               //         if (stream != null) {
+                                            streamFound = true;
+                                            totalStatted += stream;
+                                        }
+                                    }
+                                }
+                            }
+                      //  }
+                    }
+
+            System.out.println(":::::::::H:metrics:::metrics::33:::::::: " );
+*/
+
+
+
+
+            for (ExecutorDetails executor : allExecutors) {
+                     //   System.out.println(topologyId+":::allExecutors::: " + executor.toString());
+                                               // executor.
+                        //String tmp_exceuter = executor.toString();
+                    }
+
+        }
+
+
+//        Map<WorkerSlot, List<ExecutorDetails>> aliveAssigned =
+//        EvenScheduler.getAliveAssignedWorkerSlotExecutors(cluster, topology.getId());
+//        Set<WorkerSlot> canReassignSlots = slotsCanReassign(cluster, aliveAssigned.keySet());
+
+
+        //Set<WorkerSlot> slots= new WorkerSlot ;
+
+
+//        for(TopologyDetails t : topologies){
+//
+//            SupervisorDetails supervisor = supervisors.get(site);
+//            List<WorkerSlot> workerSlots = cluster.getAvailableWorkerSlots(supervisor);
+//            List<ExecutorDetails> executors = cluster.getNeedsSchedulingComponentToExecutors(t).get(name);
+//            if(!workerSlots.isEmpty() && executors != null){
+//                cluster.assign(workerSlots.get(0), t.getId(), executors);
+//            }
+//
+//
+//        }
+
+        //getSupervisorsResourcesMap
+
+
+
+
+
+
+
+
+
+     //   Set<WorkerSlot> result = new HashSet<WorkerSlot>();
+      /*  for (WorkerSlot slot : slots) {
+
+           // SupervisorDetails supervisor = cluster.getSupervisorById(slot.getNodeId());
+            //List <SupervisorDetails> supervisor = cluster.getSupervisorById();
+           // String supervisor_id = supervisor.getId() ;
+
+
+            *//*String slot_node_id= slot.getNodeId();
+            Double total_cpu =supervisor.getTotalMemory();
+            System.out.println("*****************  supervisor  for **************"+  (supervisor_id) );
+            System.out.println("*****************  supervisor  for **************"+  (slot_node_id) );
+            System.out.println("*****************  total_cpu :: supervisor *****************"+  Double.toString(total_cpu) );
+            System.out.println("---------------------------------------------------------------------------");
+*//*
+            // NormDouble ss =supervisor.getTotalResources();
+        }*/
+
+
+
+
+
+
+        //SupervisorDetails supervisor = cluster.getsu
+
+
+        // Set<ExecutorDetails> allExecutors = topology.getExecutors();
+
+       /* double num_memory_resource_2=-1 ;
+        double topology_memory_heap=0;*/
+
+
+
+
+       // SupervisorDetails supervisor = cluster.getAllScheduledResourcesForNode();
+
+
+
+
+      //  WorkerSlot nodePort = entry.getKey();
+       // List<WorkerSlot> usedSlots = cluster.getUsedSlots();
+
+
+        /*Collection <WorkerSlot> workerslot =cluster.getUsedSlots();
+         //int num_worker = 0;
+         int num_worker = workerslot.size();
+        System.out.println("::::::::::num_worker::::::::::: "+num_worker);
+
+         if (num_worker > 0)
+         {
+
+         }
+*/
+             //  MonitorScheduling monitor =new MonitorScheduling();
+            /* MonitorScheduling monitor =new MonitorScheduling();
+             monitor.call_monitor();
+             System.out.println("::::::::::Going to Monitor Scheduling  00::::::::::: " );
+             System.out.println("++++++++++++++++  before waiting ++++++++++++++++");
+
+
+             try
+             {
+                 Thread.sleep(120000);
+             }
+             catch(InterruptedException ex)
+             {
+                 Thread.currentThread().interrupt();
+             }
+
+             System.out.println("++++++++++++++++  after waiting ++++++++++++++++");
+
+
+         }
+
+
+        if (num_worker == 2 && EvenScheduler.allow_reschedule){
+
+            Map<String, SupervisorDetails> supervisordetls = cluster.getSupervisors();
+
+            for (  Map.Entry<String,  SupervisorDetails > entry : supervisordetls.entrySet()) {
+
+                String id_supervisor = entry.getKey();
+
+
+
+                System.out.println("*************id_supervisor****entry**********"+  (id_supervisor.toString()) );
+
+                SupervisorDetails supervisordetlsvue = entry.getValue();
+                System.out.println("***************************"+  (supervisordetlsvue.toString()) );
+
+            }
+//
+            System.out.println("                                                                              " );
+            System.out.println("                           Resources                                          " );
+//
+//
+            Map<String, SupervisorResources> supervisorresource = cluster.getSupervisorsResourcesMap();
+//
+            for (  Map.Entry<String,  SupervisorResources > entry : supervisorresource.entrySet()) {
+//
+                String id_supervisor_resource = entry.getKey();
+
+
+                System.out.println("*************id_supervisor_resource****entry**********"+  (id_supervisor_resource.toString()) );
+
+                SupervisorResources sp_res = entry.getValue();
+                Double TotalCpu = sp_res.getTotalCpu();
+//                Double AvailableCpu = sp_res.getAvailableCpu();
+                Double UsedCpu = sp_res.getUsedCpu();
+//                Double TotalMem = sp_res.getTotalMem();
+//                Double UsedMem = sp_res.getUsedMem();
+
+//
+//
+
+               System.out.println("***********supervisor_resource****Value************"+  sp_res.toString() );
+               System.out.println("***********supervisor_resource****TotalCpu************"+  TotalCpu.toString() );
+               if (  UsedCpu > 60) {
+
+
+                           System.out.println("@@@@@@@@@@@@@@@supervisor_resource****resource is used higher  than thereshold@@@@@@@@@@@@@"+  TotalCpu.toString() );
+                           System.out.println("++++++++++++++++  before waiting ++++++++++++++++");
+
+
+                        try
+                           {
+                               Thread.sleep(30000);
+                           }
+                        catch(InterruptedException ex)
+                           {
+                               Thread.currentThread().interrupt();
+                           }
+
+                           System.out.println("++++++++++++++++  after waiting ++++++++++++++++");
+                           EvenScheduler.check_cpu_usae=true;
+                           EvenScheduler.allow_reschedule=false;
+
+
+               }
+
+
+//            System.out.println("***********supervisor_resource****AvailableCpu************"+  (AvailableCpu.toString()) );
+              System.out.println("***********supervisor_resource****UsedCpu************"+  UsedCpu.toString() );
+//            System.out.println("***********supervisor_resource****TotalMem************"+  (TotalMem.toString()) );
+//            System.out.println("***********supervisor_resource****UsedMem************"+  (UsedMem.toString()) );
+
+//
+//
+            }
+
+
+
+        }
+
+
+
+
+       // Double totalCpuResource =cluster.getClusterTotalCpuResource();
+       // System.out.println("++++++++++++++ totalCpuResource +++++++++++"+ Double.toString(totalCpuResource));
+
+       // String  get_name_cluster =cluster.toString();
+        //double  get_used_memory =cluster.getUsedPorts();
+
+
+
+       // num_memory_resource_2 = (double) cluster.getScheduledMemoryForNode(topology.getId());
+
+       // System.out.println("get_name_cluster::"+ get_name_cluster.toString());
+       // System.out.println("getMinWorkercpu::"+ Double.toString(getMinWorkercpu));
+        //System.out.println("num_memory_resource_per_toplology::"+ Double.toString(num_memory_resource_2));
+
+
+       // Collection<String> alltopologies = topologies.getAllIds() ;
+      //  for (TopologyDetails  topologyDetails : cluster.getTopologies()) {
+
+       /* for (TopologyDetails  topology : cluster.getTopologies()) {
+
+
+            String  topologyId =topology.getId().toString();
+
+
+//            lst_topologies.add(topologyId);
+ //           System.out.println("Topology ID::" + topologyId);
+
+
+
+            //topology_memory_heap= topology.getTotalRequestedMemOnHeap();
+
+            //System.out.println(Topology_ID+":::Topology_memory_heap::"+ Double.toString(topology_memory_heap));
+
+            System.out.println(":::::::::::::::::::::::::::::::");
+
+
+            /// need schedulig
+
+           // Set<ExecutorDetails> allExecutors = topology.getExecutors();
+           // for (ExecutorDetails executor : allExecutors) {
+                //System.out.println(Topology_ID+":::allExecutors::: " + executor.toString());
+                //String tmp_exceuter = executor.toString();
+            }
+
+           // scheduleTopology(topology, cluster);
+
+//            SupervisorDetails supervisor = cluster.getSupervisorById(slot.getNodeId());
+//            String supervisor_id = supervisor.getId() ;
+//
+//
+//            String slot_node_id= slot.getNodeId();
+
+           // for (WorkerSlot port : usedports)
+            //    System.out.println("::::::::::USEDSLOTS::::::::::::: " + port.toString());
+
+
+            //////////////////////////////////////////////////////
+//            Map<ExecutorDetails, WorkerSlot> newAssignment = EvenScheduler.scheduleTopologiesEvenly(topology, cluster);
+//            Map<WorkerSlot, List<ExecutorDetails>> nodePortToExecutors = Utils.reverseMap(newAssignment);
+//
+//            for (Map.Entry<WorkerSlot, List<ExecutorDetails>> entry : nodePortToExecutors.entrySet()) {
+//                WorkerSlot nodePort = entry.getKey();
+//                List<ExecutorDetails> executors = entry.getValue();
+//                System.out.println("++++++++++++++++   scheduleTopologiesEvenly++++++nodePort+++++++"+ nodePort.toString());
+//                System.out.println("++++++++++++++++   scheduleTopologiesEvenly++++++topologyId+++++++"+ topologyId.toString());
+//                cluster.assign(port, topologyId, executors);
+//            }
+
+            //////////////////////////////////////////////////////////
+
+
+           // Map<WorkerSlot, List<ExecutorDetails>> aliveAssigned =  EvenScheduler.getAliveAssignedWorkerSlotExecutors(cluster, topology.getId());
+            //Set<WorkerSlot> canReassignSlots = slotsCanReassign(cluster, aliveAssigned.keySet());
+
+
+
+            // NimbusClient nimbusClient = NimbusClient.getConfiguredClient(config);
+
+           *//* try {
+                String s= nimbusClient.getClient().getNimbusConf()
+            } catch (AuthorizationException e) {
+                e.printStackTrace();
+            }*//*
+
+
+           *//* try (NimbusClient nimbusClient = NimbusClient.getConfiguredClient(config)) {
+
+                        String s = nimbusClient.getClient().getNimbusConf();
+
+                );
+            } catch (AuthorizationException e) {
+                e.printStackTrace();
+            }*//*
+
+
+        }
+*/
+        for(int i=0;i<availableSlots.size();i++) {
+
+            //System.out.println("AvailableSlots::" + availableSlots.get(i).toString());
+
+        }
+
+
+       /* if (num_worker  ==  1)
+        {
+            System.out.println("++++++++++++++++   ++++++++++++++++"+  Integer.toString( workerslot.size() ) );
+            if (workerslot.size() == 1   &&   (!EvenScheduler.allow_reschedule)  )
+            {
+                System.out.println("++++++++++++++++  before waiting ++++++++++++++++");
+                //sleep (10000);
+
+              *//*  try
+                {
+                    Thread.sleep(30000);
+                }
+                catch(InterruptedException ex)
+                {
+                    Thread.currentThread().interrupt();
+                }
+*//*
+
+
+
+                //EvenScheduler.scheduleTopologiesEvenly(new Topologies(topology), cluster);
+                //String topology_id= lst_topologies.get(1);
+
+//                  EvenScheduler.allow_reschedule =true;
+
+                  //  EvenScheduler.one_time_reschedule =true;
+                //EvenScheduler.scheduleTopologiesEvenlyHamid( topologies , cluster,topology_id );
+
+                System.out.println("++++++++++++++++  after waiting ++++++++++++++++");
+
+            }
+
+        }
+
+*/
+
+
+
+        System.out.println(">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>");
+        System.out.println(">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>");
+        System.out.println("                                                                     ");
+
+
+        // str_topology_name = topology.getName();
+      //  String topology_id= topology.getId();
+
+
+
+      //  topology_memory_heap= topology.getTotalRequestedMemOnHeap();
+
+
+        // num_memory_resource_2 = (int) cluster.getScheduledMemoryForNode(topology_id);
+
+
+
+         //Set<TopologyDetails> alltopologies = topologies.ge
+
+
+       //
+
+
+
+
+        /*for (String s : alltopologies) {
+            System.out.println("value= " + s.toString());
+        }*/
+
+
+    //  String s="";
+
+        /*for (Topologies  s : alltopologies) {
+            System.out.println("allExecutors::: " + executor.toString());
+            String tmp_exceuter = executor.toString();
+        }*/
+
+        //System.out.println("Topology ID::" + topology_id.toString());
+
+
+        /////// new
+        // List<WorkerSlot> workerSlots = cluster.getAvailableSlots();
+        // Set<ExecutorDetails> allExecutors = topology.getExecutors();
+
+           /* double num_memory_resource_2=-1 ;
+            String str_topology_id="";
+            double topology_memory_heap=0;
+
+            String str_cpu_resource = "";
+            String str_memory_resource = "";
+            String str_topology_name = "";
+            String str_allExecutors = "";
+
+            System.out.println("++++++++++++++++  defaultSchedule::totalSlotsToUse ++++++++++++++++"+ Integer.toString(totalSlotsToUse));
+            Double getMinWorkercpu =cluster.getMinWorkerCpu();
+
+            String  get_name_cluster =cluster.toString();
+
+            num_memory_resource_2 = (double) cluster.getScheduledMemoryForNode(topology.getId());
+
+            System.out.println("get_name_cluster::"+ get_name_cluster.toString());
+            System.out.println("getMinWorkercpu::"+ Double.toString(getMinWorkercpu));
+            System.out.println("num_memory_resource_per_toplology::"+ Double.toString(num_memory_resource_2));
+
+            str_topology_name = topology.getName();
+            String topology_id= topology.getId();
+
+
+
+            topology_memory_heap= topology.getTotalRequestedMemOnHeap();
+
+
+            //  num_memory_resource_2 = (int) cluster.getScheduledMemoryForNode(topology_id);
+
+            for(int i=0;i<availableSlots.size();i++) {
+                 System.out.println(availableSlots.get(i));
+               System.out.println("AvailableSlots::" + availableSlots.get(i).toString());
+            }
+
+            System.out.println("Topology ID::" + topology_id.toString());
+
+
+            System.out.println("Topology_memory_heap::"+ Double.toString(topology_memory_heap));
+
+            //   workerslots.forEach(System.out::println);
+
+
+            //List<Integer> list = Arrays.asList(3,2,1,4,5,6,6);
+
+            // alternative you can declare the list via:
+            // List<Integer> list = new ArrayList<>();
+            // and use list.add(element); to add elements
+
+            /// iterating a collection
+
+           for (WorkerSlot worker : workerSlots)
+                System.out.println("USEDSLOTS::: " + worker.toString());
+
+
+            for (ExecutorDetails executor : allExecutors) {
+                System.out.println("allExecutors::: " + executor.toString());
+                String tmp_exceuter = executor.toString();
+            }
+
+
+            //executor.
+//                executors.
+            for(int j=0;j<executors.size();j++){
+                // System.out.println(executors.get(j));
+                System.out.println("Executers_Details::"+executors.get(j).toString());
+                // System.out.println("Executers_Details::"+executors.get(j).
+
+            }
+*/
+        /// new
+
+    }
+
+
+
+    public static Set<WorkerSlot> slotsCanReassign(Cluster cluster, Set<WorkerSlot> slots) {
+        Set<WorkerSlot> result = new HashSet<WorkerSlot>();
+        for (WorkerSlot slot : slots) {
+  //          System.out.println("++++++++++++++ slotsCanReassign:::slot +++++++++++" +  slot.getNodeId().toString());
+            if (!cluster.isBlackListed(slot.getNodeId())) {
+                SupervisorDetails supervisor = cluster.getSupervisorById(slot.getNodeId());
+                if (supervisor != null) {
+                    Set<Integer> ports = supervisor.getAllPorts();
+                    if (ports != null && ports.contains(slot.getPort())) {
+                        result.add(slot);
+                    }
+                }
+            }
+        }
+        return result;
+    }
+
+
+
+
+    public static void call_monitor() throws Exception {
+      /*  Map<String, Object> cl = CLI.opt("i", "interval", 4, CLI.AS_INT)
+                .opt("m", "component", null)
+                .opt("s", "stream", "default")
+                .opt("w", "watch", "emitted")
+                .arg("topologyName", CLI.FIRST_WINS)
+                .parse(args);*/
+       // final org.apache.storm.utils.Monitor monitor = new org.apache.storm.utils.Monitor();
+
+      //MonitorScheduling monitor =new MonitorScheduling();
+
+
+       /* NimbusClient.withConfiguredClient(new NimbusClient.WithNimbus() {
+            @Override
+            public void run(Nimbus.Iface nimbus) throws Exception {
+                monitor.metrics(nimbus);
+            }
+        });*/
+
+    }
+
+
+
+
+
+
+
+}
Index: storm-server/src/main/java/org/apache/storm/scheduler/DefaultScheduler.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- storm-server/src/main/java/org/apache/storm/scheduler/DefaultScheduler.java	(date 1581779239000)
+++ storm-server/src/main/java/org/apache/storm/scheduler/DefaultScheduler.java	(date 1586082518089)
@@ -18,14 +18,11 @@
 
 package org.apache.storm.scheduler;
 
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Map;
-import java.util.Map.Entry;
-import java.util.Set;
 import org.apache.storm.utils.Utils;
 
+import java.util.*;
+import java.util.Map.Entry;
+
 public class DefaultScheduler implements IScheduler {
 
     private static Set<WorkerSlot> badSlots(Map<WorkerSlot, List<ExecutorDetails>> existingSlots, int numExecutors, int numWorkers) {
@@ -70,12 +67,46 @@
     }
 
     public static void defaultSchedule(Topologies topologies, Cluster cluster) {
+
+        /////// hamid
+        //   System.out.println("++++++++++++++++::Hmetrics::::display_metrics:::::: ++++++++++++++++");
+                Hmetrics hmetrics = new Hmetrics();
+                hmetrics.display_metrics(topologies, cluster);
+
+
+        //   System.out.println("++++++++++++++++::MigrationScheduler::::schedule:::::: ++++++++++++++++");
+
+              //  MigrationScheduler migrationScheduler  =new MigrationScheduler();
+              //  migrationScheduler.schedule( topologies,  cluster);
+
+
+//                List<WorkerNodeUsageDetails> workerNodeUsageDetails = null;
+//                GetWorkerNodeInfo getworkernodeInfo  =new GetWorkerNodeInfo();
+//                workerNodeUsageDetails= getworkernodeInfo.getWorkernodeInfo( topologies,  cluster);
+//
+//                for (WorkerNodeUsageDetails workernodedetails : workerNodeUsageDetails) {
+//
+//                    System.out.println("+++++++++++ ::DefaultScheduler:::: ++++++++CpuUsage+++++++++++"+workernodedetails.CpuUsage);
+//                    System.out.println("++++++++++++::DefaultScheduler::::: ++++++++MemoryUsage++++++++"+workernodedetails.MemoryUsage);
+//                    System.out.println("++++++++++++::DefaultScheduler:::::: ++++++++SupervisorID++++++"+workernodedetails.SupervisorID);
+//
+//                }
+
+
+
+                       //  GetExecuterDetails getexecuterdetails = new GetExecuterDetails();
+                       //  getexecuterdetails.getExecuterMetric(topologies, cluster);
+
+
+
+        /////// hamid
+
         for (TopologyDetails topology : cluster.needsSchedulingTopologies()) {
             List<WorkerSlot> availableSlots = cluster.getAvailableSlots();
             Set<ExecutorDetails> allExecutors = topology.getExecutors();
 
             Map<WorkerSlot, List<ExecutorDetails>> aliveAssigned =
-                EvenScheduler.getAliveAssignedWorkerSlotExecutors(cluster, topology.getId());
+                    EvenScheduler.getAliveAssignedWorkerSlotExecutors(cluster, topology.getId());
             Set<ExecutorDetails> aliveExecutors = new HashSet<ExecutorDetails>();
             for (List<ExecutorDetails> list : aliveAssigned.values()) {
                 aliveExecutors.addAll(list);
@@ -110,4 +141,4 @@
     public Map<String, Map<String, Double>> config() {
         return new HashMap<>();
     }
-}
+}
\ No newline at end of file
Index: storm-server/pom.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- storm-server/pom.xml	(date 1581779239000)
+++ storm-server/pom.xml	(date 1585511319123)
@@ -62,6 +62,12 @@
             <artifactId>rocksdbjni</artifactId>
         </dependency>
 
+        <dependency>
+            <groupId>com.jcraft</groupId>
+            <artifactId>jsch</artifactId>
+            <version>0.1.55</version>
+        </dependency>
+
         <!-- jline is included to make the zk shell work through the cli for debugging -->
         <dependency>
             <groupId>jline</groupId>
@@ -125,6 +131,16 @@
             <groupId>org.apache.httpcomponents</groupId>
             <artifactId>httpclient</artifactId>
         </dependency>
+        <dependency>
+            <groupId>com.google.guava</groupId>
+            <artifactId>guava</artifactId>
+            <version>27.0.1-jre</version>
+        </dependency>
+        <dependency>
+            <groupId>org.apache.storm</groupId>
+            <artifactId>storm-core</artifactId>
+            <version>1.2.3</version>
+        </dependency>
     </dependencies>
 
     <build>
@@ -181,7 +197,7 @@
                 <artifactId>maven-checkstyle-plugin</artifactId>
                 <!--Note - the version would be inherited-->
                 <configuration>
-                    <maxAllowedViolations>763</maxAllowedViolations>
+                    <maxAllowedViolations>5000</maxAllowedViolations>
                 </configuration>
             </plugin>
             <plugin>
Index: storm-server/src/main/java/org/apache/storm/scheduler/GetExecuterDetails.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- storm-server/src/main/java/org/apache/storm/scheduler/GetExecuterDetails.java	(date 1586072777132)
+++ storm-server/src/main/java/org/apache/storm/scheduler/GetExecuterDetails.java	(date 1586072777132)
@@ -0,0 +1,158 @@
+package org.apache.storm.scheduler;
+
+import org.apache.storm.generated.Nimbus;
+import org.apache.storm.utils.NimbusClient;
+
+import java.util.Collection;
+import java.util.HashSet;
+
+public class GetExecuterDetails {
+
+    private static final String WATCH_TRANSFERRED = "transferred";
+    private static final String WATCH_EMITTED = "emitted";
+
+    private int interval = 4;
+    private String component;
+    private static String stream;
+    //private String watch;
+    private static String watch;
+
+    public static void getExecuterMetricDmon(Topologies topologies, Cluster cluster) {
+
+        while (true)
+        {
+            try {
+                Thread.sleep(60000);
+                System.out.println("++++++++++++++++::getExecuterMetricDmon::::::::: ++++++++++++++++");
+                getExecuterMetric(topologies, cluster);
+
+            } catch (InterruptedException ex) {
+                Thread.currentThread().interrupt();
+            }
+        }
+
+
+    }
+
+
+    public static void getExecuterMetric(Topologies topologies, Cluster cluster) {
+
+        System.out.println("                                                                     ");
+       // System.out.println("++++++++++++++++::getexecuterdetails::::getExecuterMetric:::::: ++++++++++++++++");
+
+
+        Collection<WorkerSlot> workerslot =cluster.getUsedSlots();
+        //int num_worker = 0;
+        int num_worker = workerslot.size();
+        System.out.println("::::::::::num_worker::::::::::: "+num_worker);
+
+        MonitorScheduling monitor =new MonitorScheduling();
+        //monitor.call_monitor();
+
+        /// in the next step we should set this for all this procedure
+
+//        String  topologyID="";
+//        for(TopologyDetails topology : topologies) {
+//             topologyID= topology.getId();
+//        }
+
+        if (num_worker > 0) {
+            System.out.println("++++++++++++++++  before waiting ++++++++++++++++");
+            try {
+                Thread.sleep(10000);
+            } catch (InterruptedException ex) {
+                Thread.currentThread().interrupt();
+            }
+
+            System.out.println("++++++++++++++++  after waiting ++++++++++++++++");
+
+
+            Integer interval = 4;
+            // String component="sentenceGenerator";
+            String component = "";
+
+//            String stream = "default";
+//            String watch = "emitted";
+//            String topologyName = "MyTopology";
+//
+//            //monitor.setComponent(component);
+//            monitor.setStream(stream);
+//            monitor.setWatch(watch);
+//            monitor.setTopology(topologyName);
+
+            //  HashSet<String> components = monitor.getComponents(client, topology);
+            //             System.out.println("Available components for " + topology + " :");
+            //             System.out.println("------------------");
+            //             for (String comp : components) {
+            //                 System.out.println(comp);
+            //             }
+
+            try {
+
+                NimbusClient.withConfiguredClient(new NimbusClient.WithNimbus() {
+                    @Override
+                    public void run(Nimbus.Iface nimbus) throws Exception {
+                        //System.out.println("::::::::::Going to cll  11::::::::::: ");
+
+                        String topologyname = "";
+                        String topologyID="";
+
+                        if ( num_worker == 0)  return;
+
+                        for (TopologyDetails topology : cluster.getTopologies()) {
+                            topologyname = topology.getName().toString();
+                            topologyID =topology.getId();
+                        }
+                        //System.out.println("+++++++++topologyID::::: "+ topologyID );
+
+                        HashSet<String> components = monitor.getComponents(nimbus, topologyname);
+                        System.out.println("Available components for " + topologyname + " :");
+                        System.out.println("------------------");
+                        for (String comp : components) {
+
+                            if (!comp.startsWith("__acker")  && !comp.startsWith("log")) {
+
+                                MonitorScheduling monitor =new MonitorScheduling();
+                                //System.out.println(comp);
+                                //String component =comp.toString();
+                                // String component="sentenceGenerator";
+                                //String componetID = comp.com
+
+                               // monitor.setComponent(comp);
+                                String stream = "default";
+                                String watch = "emitted";
+                                String topologyName = "MyTopology";
+
+
+                                monitor.setComponent(comp);
+                                monitor.setStream(stream);
+                                monitor.setWatch(watch);
+                                monitor.setTopology(topologyName);
+                                //System.out.println("+++++++++topologyID::::: "+ topologyID );
+                                monitor.setTopologyID(topologyID);
+
+
+                                monitor.metrics(nimbus);
+                                System.out.println("------------------");
+//                                try {
+//                                    Thread.sleep(6000);
+//                                } catch (InterruptedException ex) {
+//                                    Thread.currentThread().interrupt();
+//                                }
+
+                           }
+
+                       }
+
+
+                    }
+                });
+            } catch (Exception e) {
+                e.printStackTrace();
+            }
+
+        }
+    }
+
+
+}
Index: storm-core/src/jvm/org/apache/storm/utils/Monitor.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- storm-core/src/jvm/org/apache/storm/utils/Monitor.java	(date 1581779239000)
+++ storm-core/src/jvm/org/apache/storm/utils/Monitor.java	(date 1584557549881)
@@ -108,6 +108,7 @@
 
         int componentParallelism = 0;
         boolean streamFound = false;
+        System.out.println("::::::::::metrics::::::::::::: " );
         ClusterSummary clusterSummary = client.getClusterInfo();
         TopologySummary topologySummary = null;
         for (TopologySummary ts : clusterSummary.get_topologies()) {
@@ -123,6 +124,7 @@
             GetInfoOptions getInfoOpts = new GetInfoOptions();
             getInfoOpts.set_num_err_choice(NumErrorsChoice.NONE);
             TopologyInfo info = client.getTopologyInfoWithOpts(id, getInfoOpts);
+
             for (ExecutorSummary es : info.get_executors()) {
                 if (component.equals(es.get_component_id())) {
                     componentParallelism++;
@@ -143,6 +145,8 @@
                     }
                 }
             }
+
+
         }
 
         if (componentParallelism <= 0) {
